# Bolt-XL AutoAWQ Server (CUDA build)

FROM nvidia/cuda:12.8.0-devel-ubuntu22.04

RUN apt-get update && apt-get install -y \
    git \
    build-essential \
    cmake \
    ninja-build \
    python3 \
    python3-dev \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

ENV CUDA_HOME=/usr/local/cuda
ENV TORCH_CUDA_ARCH_LIST=12.0+PTX
ENV CMAKE_CUDA_ARCHITECTURES=120
ENV COMPUTE_CAPABILITIES=75,80,86,87,89,90,120

COPY requirements.txt .
RUN python3 -m pip install --no-cache-dir -r requirements.txt
RUN python3 -m pip install --no-cache-dir --no-deps autoawq==0.2.6
RUN git clone --depth 1 https://github.com/casper-hansen/AutoAWQ_kernels.git /tmp/autoawq_kernels \
    && sed -i 's/{80, 86, 89, 90}/{80, 86, 89, 90, 120}/' /tmp/autoawq_kernels/setup.py \
    && python3 -m pip install --no-cache-dir --no-deps /tmp/autoawq_kernels \
    && rm -rf /tmp/autoawq_kernels

COPY server.py .

ENV HF_HOME=/models/hf
ENV TRANSFORMERS_CACHE=/models/hf
ENV HUGGINGFACE_HUB_CACHE=/models/hf
ENV TOKENIZERS_PARALLELISM=false

EXPOSE 3000

CMD ["uvicorn", "server:app", "--host", "0.0.0.0", "--port", "3000"]
