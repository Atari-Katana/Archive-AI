version: '3.8'

services:
  # 1. The State Engine (Redis Stack)
  redis:
    image: redis/redis-stack:latest
    environment:
      - REDIS_ARGS=--maxmemory 20gb --maxmemory-policy allkeys-lru --appendonly yes --protected-mode no
    ports:
      - "6379:6379"      # Redis
      - "8002:8001"      # RedisInsight UI
    volumes:
      - ./data/redis:/data
    deploy:
      resources:
        limits:
          memory: 24G  # Docker limit slightly higher than Redis config
    networks:
      - archive-net

  # 2. Engine A: Speed (Vorpal)
  vorpal:
    build: ./vorpal
    image: archive-ai/vorpal:latest
    environment:
      - GPU_MEMORY_UTILIZATION=${VORPAL_GPU_MEMORY_UTILIZATION:-0.60}  # ~9.6GB (Goblin on CPU now)
      - CUDA_VISIBLE_DEVICES=0
    ports:
      - "8000:8000"
    volumes:
      - ./models/vorpal:/models
    networks:
      - archive-net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command:
      - "${VORPAL_MODEL:-Qwen/Qwen2.5-3B-Instruct}"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--gpu-memory-utilization"
      - "${VORPAL_GPU_MEMORY_UTILIZATION:-0.60}"
      - "--max-model-len"
      - "${VORPAL_MAX_MODEL_LEN:-8192}"

  # 3. Engine B: Capacity (Goblin) - CPU-only for reasoning/coding
  goblin:
    image: ghcr.io/ggml-org/llama.cpp:server  # CPU-only version
    environment:
      - MODEL_PATH=${GOBLIN_MODEL_PATH:-/models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf}
      - N_GPU_LAYERS=0  # CPU-only (no GPU layers)
      - CTX_SIZE=${GOBLIN_CTX_SIZE:-8192}
      - HOST=0.0.0.0
      - PORT=8080
    ports:
      - "8081:8080"  # Map to 8081 externally (brain uses 8080)
    volumes:
      - ./models/goblin:/models
    networks:
      - archive-net
    deploy:
      resources:
        limits:
          memory: 8G  # Limit to 8GB RAM
    command:
      - "--model"
      - "${GOBLIN_MODEL_PATH:-/models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf}"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8080"
      - "--ctx-size"
      - "${GOBLIN_CTX_SIZE:-8192}"
      - "--n-gpu-layers"
      - "0"
      - "--threads"
      - "${GOBLIN_THREADS:-16}"  # Increased for CPU performance
      - "--batch-size"
      - "${GOBLIN_BATCH_SIZE:-512}"
      - "--ubatch-size"
      - "${GOBLIN_UBATCH_SIZE:-512}"
      - "--cont-batching"
      - "--metrics"

  # 4. The Brain (Orchestrator)
  brain:
    build: ./brain
    image: archive-ai/brain:latest
    depends_on:
      - redis
      - vorpal
    environment:
      - REDIS_URL=redis://redis:6379
      - VORPAL_MODEL=${VORPAL_MODEL:-Qwen/Qwen2.5-3B-Instruct}
      - VORPAL_URL=http://vorpal:8000
      - GOBLIN_URL=http://goblin:8080
      - SANDBOX_URL=http://sandbox:8000
      - VOICE_URL=http://voice:8001
      - ASYNC_MEMORY=true  # Enabled in Chunk 2.3
      - NVIDIA_VISIBLE_DEVICES=0  # Access for monitoring only
    ports:
      - "8080:8080"
    volumes:
      - ./ui:/app/ui  # Mount UI files for static serving
    networks:
      - archive-net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [utility]  # utility capability for nvidia-smi

  # 4.5. Code Execution Sandbox
  sandbox:
    build: ./sandbox
    image: archive-ai/sandbox:latest
    ports:
      - "8003:8000"
    networks:
      - archive-net
    restart: unless-stopped

  # 5. Voice Service (Faster-Whisper STT + F5-TTS)
  voice:
    build: ./voice
    image: archive-ai/voice:latest
    environment:
      - WHISPER_MODEL=base  # tiny, base, small, medium, large
      - WHISPER_DEVICE=cpu  # cpu or cuda
      - WHISPER_COMPUTE_TYPE=int8  # int8, float16, float32
      - TTS_DEVICE=cpu  # cpu or cuda
      - TTS_MODEL_CACHE=/models/cache
    ports:
      - "8001:8001"
    volumes:
      - ./models/whisper:/root/.cache/huggingface  # Cache for Whisper models
      - ./models/f5-tts:/models/cache  # F5-TTS models cache
    networks:
      - archive-net

  # 6. Librarian Service (Library Ingestion)
  librarian:
    build: ./librarian
    image: archive-ai/librarian:latest
    volumes:
      - ~/ArchiveAI/Library-Drop:/watch  # Watch directory for new files
      - ./data/library:/data  # Processed chunks storage (for debugging)
    networks:
      - archive-net
    restart: unless-stopped

  # 7. MCP Server for Autonomous Agents (DISABLED - not implemented)
  # mcp-server:
  #   build: ./mcp-server
  #   image: archive-ai/mcp-server:latest
  #   environment:
  #     - OPENAI_API_KEY=${OPENAI_API_KEY:-}
  #     - LLM_MODEL=${LLM_MODEL:-gpt-4}
  #     - LLM_PROVIDER=${LLM_PROVIDER:-openai}
  #     - MAX_ITERATIONS_DEFAULT=10
  #     - DASHBOARD_PORT=8000
  #     - MCP_SERVER_PORT=8001
  #     - AUTOGPT_ENABLED=true
  #   ports:
  #     - "8003:8000"  # Dashboard
  #     - "8004:8001"  # MCP Server
  #   volumes:
  #     - ./mcp-server/logs:/app/logs
  #   networks:
  #     - archive-net
    restart: unless-stopped

networks:
  archive-net:
    driver: bridge
