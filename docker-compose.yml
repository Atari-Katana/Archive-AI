version: '3.8'

services:
  # 1. The State Engine (Redis Stack)
  redis:
    image: redis/redis-stack:latest
    environment:
      - REDIS_ARGS=--maxmemory 20gb --maxmemory-policy allkeys-lru --appendonly yes --protected-mode no
    ports:
      - "${REDIS_PORT:-6379}:6379"      # Redis
      - "${REDIS_UI_PORT:-8002}:8001"      # RedisInsight UI
    volumes:
      - ./data/redis:/data
    deploy:
      resources:
        limits:
          memory: 24G  # Docker limit slightly higher than Redis config
    networks:
      - archive-net

  # 2. Engine A: Speed (Vorpal) - CPU-only fallback
  vorpal:
    image: ghcr.io/ggml-org/llama.cpp:server
    environment:
      - MODEL_PATH=${VORPAL_MODEL_PATH:-/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf}
      - N_GPU_LAYERS=0
      - CTX_SIZE=${VORPAL_CTX_SIZE:-4096}
      - HOST=0.0.0.0
      - PORT=8000
    ports:
      - "${VORPAL_PORT:-8000}:8000"
    volumes:
      - ./models/vorpal:/models
    networks:
      - archive-net
    deploy:
      resources:
        limits:
          memory: 8G
    command:
      - "--model"
      - "${VORPAL_MODEL_PATH:-/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf}"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--ctx-size"
      - "${VORPAL_CTX_SIZE:-4096}"
      - "--n-gpu-layers"
      - "0"
      - "--threads"
      - "${VORPAL_THREADS:-8}"
      - "--batch-size"
      - "${VORPAL_BATCH_SIZE:-512}"
      - "--ubatch-size"
      - "${VORPAL_UBATCH_SIZE:-512}"
      - "--cont-batching"
      - "--metrics"

  # 3. Engine B: Capacity (Goblin) - CPU-only for reasoning/coding
  goblin:
    image: ghcr.io/ggml-org/llama.cpp:server  # CPU-only version
    environment:
      - MODEL_PATH=${GOBLIN_MODEL_PATH:-/models/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf}
      - N_GPU_LAYERS=0  # CPU-only (no GPU layers)
      - CTX_SIZE=${GOBLIN_CTX_SIZE:-8192}
      - HOST=0.0.0.0
      - PORT=8080
    ports:
      - "${GOBLIN_PORT:-8082}:8080"  # Map to 8082 externally (brain uses 8081)
    volumes:
      - ./models/goblin:/models
    networks:
      - archive-net
    deploy:
      resources:
        limits:
          memory: 8G  # Limit to 8GB RAM
    command:
      - "--model"
      - "${GOBLIN_MODEL_PATH:-/models/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf}"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8080"
      - "--ctx-size"
      - "${GOBLIN_CTX_SIZE:-8192}"
      - "--n-gpu-layers"
      - "0"
      - "--threads"
      - "${GOBLIN_THREADS:-16}"  # Increased for CPU performance
      - "--batch-size"
      - "${GOBLIN_BATCH_SIZE:-512}"
      - "--ubatch-size"
      - "${GOBLIN_UBATCH_SIZE:-512}"
      - "--cont-batching"
      - "--metrics"

  # 3b. Engine C: Bolt-XL (AutoAWQ)
  bolt-xl:
    build:
      context: ./bolt-xl
      dockerfile: Dockerfile.transformers
    container_name: archive-bolt-xl
    environment:
      - BOLT_MODEL=${BOLT_MODEL:-google/gemma-2b-it}
      - BOLT_DTYPE=${BOLT_DTYPE:-float16}
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/models/hf
      - TRANSFORMERS_CACHE=/models/hf
      - HUGGINGFACE_HUB_CACHE=/models/hf
      - BOLT_MAX_NEW_TOKENS=${BOLT_MAX_NEW_TOKENS:-1024}
    runtime: nvidia
    ports:
      - "${BOLT_XL_PORT:-8007}:3000"   # Host: 8007 -> Internal: 3000
    volumes:
      - ./models:/models
    networks:
      - archive-net
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # 4. The Brain (Orchestrator)
  brain:
    build: ./brain
    image: archive-ai/brain:latest
    depends_on:
      - redis
      - vorpal
    environment:
      - REDIS_URL=redis://redis:6379
      - VORPAL_MODEL=${VORPAL_MODEL:-Llama-3.2-3B-Instruct}
      - VORPAL_URL=http://vorpal:8000
      - GOBLIN_URL=http://goblin:8080
      - BOLT_XL_URL=http://bolt-xl:3000
      - SANDBOX_URL=http://sandbox:8000
      - VOICE_URL=http://voice:8000
      - ASYNC_MEMORY=true  # Enabled in Chunk 2.3
    ports:
      - "${BRAIN_PORT:-8080}:8000"
    volumes:
      - ./ui:/app/ui  # Mount UI files for static serving
      - ./data:/app/data # Mount data directory for persistence
    networks:
      - archive-net

  # 4.5. Code Execution Sandbox
  sandbox:
    build: ./sandbox
    image: archive-ai/sandbox:latest
    ports:
      - "${SANDBOX_PORT:-8003}:8000"
    networks:
      - archive-net
    restart: unless-stopped

  # 5. Voice Service (Faster-Whisper STT + F5-TTS)
  voice:
    build: ./voice
    image: archive-ai/voice:latest
    environment:
      - WHISPER_MODEL=${WHISPER_MODEL:-base}  # tiny, base, small, medium, large
      - WHISPER_DEVICE=${WHISPER_DEVICE:-cuda}  # cpu or cuda
      - WHISPER_COMPUTE_TYPE=${WHISPER_COMPUTE_TYPE:-float16}  # int8, float16, float32
      - TTS_DEVICE=${TTS_DEVICE:-cuda}  # cpu or cuda
      - TTS_MODEL_PATH=${TTS_MODEL_PATH:-/app/models/xtts}
      - TTS_MODEL_CACHE=/models/cache
      - NVIDIA_VISIBLE_DEVICES=0
    ports:
      - "${VOICE_PORT:-8001}:8000"
    volumes:
      - ./models/whisper:/root/.cache/huggingface  # Cache for Whisper models
      - ./models/f5-tts:/models/cache  # F5-TTS models cache
      - ./models/xtts:/app/models/xtts  # XTTS local model files
    networks:
      - archive-net
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # 6. Librarian Service (Library Ingestion)
  librarian:
    build: ./librarian
    image: archive-ai/librarian:latest
    volumes:
      - ~/ArchiveAI/Library-Drop:/watch  # Watch directory for new files
      - ./data/library:/data  # Processed chunks storage (for debugging)
    networks:
      - archive-net
    restart: unless-stopped

  # 7. Bifrost Gateway
  bifrost:
    image: maximhq/bifrost:latest
    container_name: archive-bifrost
    ports:
      - "${BIFROST_PORT:-8081}:8080"
    volumes:
      - ./config/bifrost-config.json:/app/data/config.json
    networks:
      - archive-net
    restart: unless-stopped

  # 8. MCP Server for Autonomous Agents (DISABLED - not implemented)
  # mcp-server:
  #   build: ./mcp-server
  #   image: archive-ai/mcp-server:latest
  #   environment:
  #     - OPENAI_API_KEY=${OPENAI_API_KEY:-}
  #     - LLM_MODEL=${LLM_MODEL:-gpt-4}
  #     - LLM_PROVIDER=${LLM_PROVIDER:-openai}
  #     - MAX_ITERATIONS_DEFAULT=10
  #     - DASHBOARD_PORT=8000
  #     - MCP_SERVER_PORT=8001
  #     - AUTOGPT_ENABLED=true
  #   ports:
  #     - "8003:8000"  # Dashboard
  #     - "8004:8001"  # MCP Server
  #   volumes:
  #     - ./mcp-server/logs:/app/logs
  #   networks:
  #     - archive-net
    restart: unless-stopped

networks:
  archive-net:
    driver: bridge
