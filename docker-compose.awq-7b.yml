version: '3.8'

# Optional override to run Vorpal with a 7B AWQ model and Goblin with a 7B GGUF.
# Use with: docker-compose -f docker-compose.yml -f docker-compose.awq-7b.yml up -d vorpal goblin brain

services:
  vorpal:
    environment:
      - VORPAL_MODEL=${VORPAL_MODEL:-/models/Meta-Llama-3.1-8B-Instruct-AWQ-INT4}
      - VORPAL_GPU_MEMORY_UTILIZATION=${VORPAL_GPU_MEMORY_UTILIZATION:-0.45}  # Aim to keep Vorpal around ~6GB
      - VORPAL_MAX_MODEL_LEN=${VORPAL_MAX_MODEL_LEN:-4096}
      - VORPAL_MAX_NUM_SEQS=${VORPAL_MAX_NUM_SEQS:-64}
    command:
      - "${VORPAL_MODEL:-/models/Meta-Llama-3.1-8B-Instruct-AWQ-INT4}"   # Local AWQ model path
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--gpu-memory-utilization"
      - "${VORPAL_GPU_MEMORY_UTILIZATION:-0.45}"
      - "--max-model-len"
      - "${VORPAL_MAX_MODEL_LEN:-4096}"
      - "--max-num-seqs"
      - "${VORPAL_MAX_NUM_SEQS:-64}"

  goblin:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    environment:
      - MODEL_PATH=${GOBLIN_MODEL_PATH:-/models/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf}
      - N_GPU_LAYERS=${GOBLIN_N_GPU_LAYERS:-38}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command:
      - "--model"
      - "${GOBLIN_MODEL_PATH:-/models/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf}"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8080"
      - "--ctx-size"
      - "${GOBLIN_CTX_SIZE:-8192}"
      - "--n-gpu-layers"
      - "${GOBLIN_N_GPU_LAYERS:-38}"
      - "--threads"
      - "${GOBLIN_THREADS:-8}"
      - "--batch-size"
      - "${GOBLIN_BATCH_SIZE:-512}"
      - "--ubatch-size"
      - "${GOBLIN_UBATCH_SIZE:-512}"
      - "--flash-attn"
      - "on"
      - "--cont-batching"
      - "--metrics"

  brain:
    environment:
      - VORPAL_MODEL=${VORPAL_MODEL:-/models/Meta-Llama-3.1-8B-Instruct-AWQ-INT4}
